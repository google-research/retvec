{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a TF Lite model with RETVec\n",
    "\n",
    "Please note that using RETVec with TF Lite requires `tensorflow_text>=2.13` and `tensorflow>=2.13`. You can upgrade your TensorFlow following the instructions [here](https://www.tensorflow.org/install/pip).\n",
    "\n",
    "This notebook shows how to create, save, and run a TF Lite compatible model which uses the RETVec tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 18:50:50.441625: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-12 18:50:50.504321: I tensorflow/core/platform/cpu_feature_guard.cc:183] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# installing retvec if needed\n",
    "try:\n",
    "    import retvec\n",
    "except ImportError:\n",
    "    !pip install retvec\n",
    "\n",
    "try:\n",
    "    import tensorflow_text\n",
    "except ImportError:\n",
    "    !pip install tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # silence TF INFO messages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# import the RETVec tokenizer layer\n",
    "from retvec.tf import RETVecTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only important change to make for RETVec is to set `use_tf_lite_compatible_ops=True`. This will make the layer use `tensorflow_text.utf8_binarize` and whitespace splitting to split text into words, which is supported natively by TF Lite.\n",
    "\n",
    "Note that in this example we use a simple dense model to show conversion, since LSTM layers [require additional effort to convert to TF Lite](https://www.tensorflow.org/lite/models/convert/rnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 18:50:56.016008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 56575 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:18:00.0, compute capability: 9.0\n",
      "2023-10-12 18:50:56.018865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 78147 MB memory:  -> device: 1, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:2a:00.0, compute capability: 9.0\n",
      "2023-10-12 18:50:56.021108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 78147 MB memory:  -> device: 2, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:3a:00.0, compute capability: 9.0\n",
      "2023-10-12 18:50:56.024078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 78147 MB memory:  -> device: 3, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:5d:00.0, compute capability: 9.0\n",
      "2023-10-12 18:50:56.026126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 78147 MB memory:  -> device: 4, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:9a:00.0, compute capability: 9.0\n",
      "2023-10-12 18:50:56.028665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 78147 MB memory:  -> device: 5, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:ab:00.0, compute capability: 9.0\n",
      "2023-10-12 18:50:56.031152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 78147 MB memory:  -> device: 6, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:ba:00.0, compute capability: 9.0\n",
      "2023-10-12 18:50:56.033668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1636] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 78147 MB memory:  -> device: 7, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:db:00.0, compute capability: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 1)]               0         \n",
      "                                                                 \n",
      " ret_vec_tokenizer (RETVecT  (None, 128, 256)          230144    \n",
      " okenizer)                                                       \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128, 256)          65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128, 64)           16448     \n",
      "                                                                 \n",
      " output (Dense)              (None, 128, 4)            260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 312644 (1.19 MB)\n",
      "Trainable params: 82500 (322.27 KB)\n",
      "Non-trainable params: 230144 (899.00 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# using strings directly requires to put a shape of (1,) and dtype tf.string\n",
    "inputs = layers.Input(shape=(1, ), name=\"input\", dtype=tf.string)\n",
    "\n",
    "# add RETVec tokenizer layer with `use_tf_lite_compatible_ops`\n",
    "x = RETVecTokenizer(model='retvec-v1', use_tf_lite_compatible_ops=True)(inputs)\n",
    "\n",
    "# build the rest of the model as usual\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(4, activation='sigmoid', name=\"output\")(x)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <retvec.tf.layers.binarizer.RETVecBinarizer object at 0x7fb6df89a590>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7fb4d0064820>, because it is not built.\n",
      "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.spatial_dropout1d.SpatialDropout1D object at 0x7fb4d0064910>, because it is not built.\n",
      "INFO:tensorflow:Assets written to: ./demo_models/tf_lite_retvec/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./demo_models/tf_lite_retvec/assets\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "save_path = \"./demo_models/tf_lite_retvec\"\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the model and run inference in TF Lite\n",
    "\n",
    "We can now convert the model to a TF Lite model following the [instructions](https://www.tensorflow.org/lite/models/convert). For more information on how to use TensorFlow Lite, please see the [guide](https://www.tensorflow.org/lite/guide)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 18:51:02.343076: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "2023-10-12 18:51:02.343096: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2023-10-12 18:51:02.343590: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./demo_models/tf_lite_retvec\n",
      "2023-10-12 18:51:02.347147: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n",
      "2023-10-12 18:51:02.347160: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./demo_models/tf_lite_retvec\n",
      "2023-10-12 18:51:02.366949: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2023-10-12 18:51:02.369765: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2023-10-12 18:51:02.440370: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: ./demo_models/tf_lite_retvec\n",
      "2023-10-12 18:51:02.468173: I tensorflow/cc/saved_model/loader.cc:334] SavedModel load for tags { serve }; Status: success: OK. Took 124582 microseconds.\n",
      "2023-10-12 18:51:02.518189: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-12 18:51:02.745186: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:2084] The following operation(s) need TFLite custom op implementation(s):\n",
      "Custom ops: RaggedTensorToTensor, TFText>Utf8Binarize, TFText>WhitespaceTokenizeWithOffsetsV2\n",
      "Details:\n",
      "\ttf.RaggedTensorToTensor(tensor<3xi32>, tensor<?x384xf32>, tensor<f32>, tensor<?xi64>) -> (tensor<?x128x384xf32>) : {T = f32, Tindex = i64, Tshape = i32, device = \"\", num_row_partition_tensors = 1 : i64, row_partition_types = [\"ROW_SPLITS\"]}\n",
      "\ttf.TFText>Utf8Binarize(tensor<?x!tf_type.string>) -> (tensor<?x384xf32>) : {bits_per_char = 24 : i64, device = \"\", replacement_char = 65533 : i64, word_length = 16 : i64}\n",
      "\ttf.TFText>WhitespaceTokenizeWithOffsetsV2(tensor<?x!tf_type.string>, tensor<!tf_type.string>) -> (tensor<?x!tf_type.string>, tensor<?xi64>, tensor<?xi32>, tensor<?xi32>) : {device = \"\"}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_custom\n"
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(save_path) # path to the SavedModel directory\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "]\n",
    "converter.allow_custom_ops = True\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.lite.python import interpreter\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "# create TF lite interpreter with TF Text ops registered\n",
    "interp = interpreter.InterpreterWithCustomOps(\n",
    "    model_content=tflite_model,\n",
    "    custom_op_registerers=tf_text.tflite_registrar.SELECT_TFTEXT_OPS)\n",
    "interp.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Lite result =  [[[0.46520743 0.5190651  0.3716683  0.43701836]\n",
      "  [0.6337548  0.42784083 0.5022397  0.55659497]\n",
      "  [0.53433377 0.53684425 0.42378557 0.4369351 ]\n",
      "  [0.406101   0.5063563  0.41558668 0.31651068]\n",
      "  [0.55106455 0.54234135 0.49299878 0.23038922]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]\n",
      "  [0.4838743  0.53117436 0.36593238 0.43153659]]]\n"
     ]
    }
   ],
   "source": [
    "# run inference with our model\n",
    "input_data = np.array(['This is an example text'])\n",
    "\n",
    "tokenize = interp.get_signature_runner('serving_default')\n",
    "output = tokenize(input=input_data)\n",
    "print('TensorFlow Lite result = ', output['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
